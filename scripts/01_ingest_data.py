"""
FASE 1: INGESTA Y VALIDACI√ìN DE DATOS
Simula la lectura desde HDFS y validaci√≥n inicial
"""
import logging
import sys
from datetime import datetime
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import col, trim

# Configurar paths
sys.path.append('..')
from utils import create_spark_session, stop_spark_session, generate_validation_report
from utils.constants import RAW_TRANSACTIONS_FILE, VALIDATED_DATA_PATH, LOGS_DIR

# Configurar logging
log_file = LOGS_DIR / f"01_ingest_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

def define_schema():
    """Define el esquema expl√≠cito para el CSV"""
    return StructType([
        StructField("transaction_id", IntegerType(), False),
        StructField("customer_id", IntegerType(), False),
        StructField("account_type", StringType(), False),
        StructField("country", StringType(), False),
        StructField("amount", DoubleType(), False),
        StructField("currency", StringType(), False),
        StructField("category", StringType(), False),
        StructField("channel", StringType(), False),
        StructField("timestamp", StringType(), False),
        StructField("status", StringType(), False),
        StructField("merchant_id", StringType(), False),
        StructField("card_last4", StringType(), False)
    ])

def clean_data(df):
    """Aplica limpieza b√°sica a los datos"""
    logging.info("Aplicando limpieza de datos...")
    
    # Trim espacios en strings
    string_columns = [field.name for field in df.schema.fields 
                     if isinstance(field.dataType, StringType)]
    
    for column in string_columns:
        df = df.withColumn(column, trim(col(column)))
    
    # Eliminar duplicados por transaction_id
    initial_count = df.count()
    df = df.dropDuplicates(['transaction_id'])
    final_count = df.count()
    
    if initial_count != final_count:
        logging.warning(f"‚ö†Ô∏è Se eliminaron {initial_count - final_count} duplicados")
    
    # Eliminar registros con valores nulos en campos cr√≠ticos
    critical_fields = ['transaction_id', 'customer_id', 'amount', 'timestamp', 'status']
    df = df.na.drop(subset=critical_fields)
    
    logging.info(f"‚úÖ Limpieza completada. Registros finales: {df.count()}")
    
    return df

def main():
    """Funci√≥n principal de ingesta"""
    logging.info("=" * 70)
    logging.info("INICIANDO FASE 1: INGESTA Y VALIDACI√ìN DE DATOS")
    logging.info("=" * 70)
    
    spark = None
    
    try:
        # Crear SparkSession
        spark = create_spark_session("01_IngestData")
        
        # Verificar archivo de entrada
        if not RAW_TRANSACTIONS_FILE.exists():
            raise FileNotFoundError(f"‚ùå Archivo no encontrado: {RAW_TRANSACTIONS_FILE}")
        
        logging.info(f"üìÇ Leyendo datos desde: {RAW_TRANSACTIONS_FILE}")
        
        # Leer CSV con esquema expl√≠cito
        schema = define_schema()
        df = spark.read.csv(
            str(RAW_TRANSACTIONS_FILE),
            header=True,
            schema=schema
        )
        
        logging.info(f"‚úÖ Datos cargados. Total registros: {df.count()}")
        
        # Mostrar muestra de datos
        logging.info("\nüìä Muestra de datos (primeras 5 filas):")
        df.show(5, truncate=False)
        
        # Validar datos
        validation_report = generate_validation_report(df)
        
        # Limpiar datos
        df_clean = clean_data(df)
        
        # Guardar en formato Parquet (simulando escritura a HDFS)
        output_path = VALIDATED_DATA_PATH / "transactions_validated.parquet"
        logging.info(f"üíæ Guardando datos validados en: {output_path}")
        
        df_clean.write.mode("overwrite").parquet(str(output_path))
        
        # Verificar escritura
        df_verify = spark.read.parquet(str(output_path))
        logging.info(f"‚úÖ Verificaci√≥n: {df_verify.count()} registros guardados correctamente")
        
        # Resumen final
        logging.info("\n" + "=" * 70)
        logging.info("RESUMEN DE INGESTA")
        logging.info("=" * 70)
        logging.info(f"Registros originales: {validation_report['total_records']}")
        logging.info(f"Registros validados: {df_clean.count()}")
        logging.info(f"Duplicados eliminados: {validation_report['duplicates']}")
        logging.info(f"Output: {output_path}")
        logging.info("=" * 70)
        
        logging.info("‚úÖ FASE 1 COMPLETADA EXITOSAMENTE")
        
        return 0
        
    except Exception as e:
        logging.error(f"‚ùå Error en la ingesta: {str(e)}", exc_info=True)
        return 1
        
    finally:
        if spark:
            stop_spark_session(spark)

if __name__ == "__main__":
    sys.exit(main())